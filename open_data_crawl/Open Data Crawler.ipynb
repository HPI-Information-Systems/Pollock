{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea68eddf",
   "metadata": {},
   "source": [
    "## Notebook for statistics of availability of file types:\n",
    "\n",
    "In this notebook we sample the availability of CSV files from 17 different countries from the 5 different continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfc3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./raw_data/\"):\n",
    "    os.makedirs(\"./raw_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0833bd7",
   "metadata": {},
   "source": [
    "### North America: USA, Canada, Mexico\n",
    "\n",
    "- Usa statistics from catalog.data.gov\n",
    "- Canadas statistics from https://search.open.canada.ca\n",
    "- Mexico statistics from https://datos.gob.mx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eec561b",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <p>Statistics are in a tiny table on the left of the data.gov webpage. To obtain the total amount of datasets we query for the empty string \"\". Even though the page only shows the first 20 or so, we will crawl the information we need from the left table. To make sure the table contains all formats, we pass the parameter ``&_res_format_limit=0`` to the API.\n",
    "</p>\n",
    "    </div>\n",
    "    <div>\n",
    "        <img src=\"usa_formats.png\" align:right>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 321550\n"
     ]
    }
   ],
   "source": [
    "query = \"https://catalog.data.gov/dataset/?q=&_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "num_text = soup.find(\"div\", {\"class\": \"new-results\"})\n",
    "num_datasets = int(num_text.text.split()[0].replace(\",\", \"\"))\n",
    "print(\"Total number of datasets\", num_datasets)\n",
    "# parse the list of formats with their  number from the <nav> with aria-label \"Formats\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24083f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data.gov', 'country': 'USA', 'num_datasets': 321550, 'HTML': 167118, 'XML': 98080, 'PDF': 56947, 'CSV': 36146, 'TIFF': 35423, 'ZIP': 30243, 'TEXT': 29626, 'XYZ': 23161, 'KML': 20614, 'ArcGIS GeoServices REST API': 20285, 'JSON': 16842, 'GeoJSON': 16293, 'SID': 12835, 'JPEG': 11967, 'WMS': 11843, 'RDF': 11663, 'sos': 9335, 'EXCEL': 7948, 'Esri REST': 5911, 'NETCDF': 5308, 'WCS': 4880, 'application/unknown': 4388, 'PNG': 4381, 'WFS': 2998, 'QGIS': 2403, 'gml': 2371, 'API': 1617, 'application/vnd.geo+json': 1331, 'EXE': 1228, 'DOC': 1213, 'ArcGIS Online Map': 927, 'CDF': 896, 'application/html': 503, 'BIN': 249, 'GIF': 220, 'TAR': 219, 'Undefined': 209, 'GZ': 208, 'nc ': 197, 'chemical/x-mdl-sdfile': 192, '00': 174, 'POWERPOINT': 153, 'ArcGIS Map Preview': 149, 'ArcGIS Map Service': 149, 'ACCESS': 138, 'cdfnc ': 91, 'b0': 89, 'application/txt': 72, 'image/x-3ds': 65, 'Data Explorer': 63}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"data.gov\", \"country\": \"USA\", \"num_datasets\": num_datasets}\n",
    "for li in soup.find(\"nav\", {\"aria-label\": \"Formats\"}).find_all(\"li\"):\n",
    "    format = li.find(\"span\", {\"class\": \"item-label\"}).text\n",
    "    formats[format] = int(li.find(\"span\", {\"class\": \"item-count badge\"}).text.replace(\",\",\"\"))\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/usa_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 34075\n"
     ]
    }
   ],
   "source": [
    "#repeat for canada\n",
    "query = \"https://search.open.canada.ca/en/od\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "num_text = soup.find(\"p\", text=lambda text: \"Found\" in text)\n",
    "num_datasets = int(''.join(filter(str.isdigit, num_text.text)))\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'open.canada.ca', 'country': 'Canada', 'num_datasets': 34075, ' ASCII Grid ': 262, ' AVI ': 2, ' BAG ': 2, ' CDR ': 91, ' CSV ': 12139, ' DBF ': 1, ' DOC ': 75, ' DOCX ': 236, ' DXF ': 13, ' E00 ': 5, ' ECW ': 64, ' EDI ': 1881, ' ESRI REST ': 1471, ' EXE ': 3, ' FGDB/GDB ': 661, ' Flat raster binary ': 1, ' GDB ': 9, ' GEOJSON ': 715, ' GIF ': 97, ' GML ': 313, ' GPKG ': 106, ' GRD ': 2, ' GRIB2 ': 10, ' GeoPDF ': 1, ' GeoTIF ': 249, ' HDF ': 3, ' HTML ': 20041, ' IATI ': 1, ' JAR ': 1, ' JP2 ': 1227, ' JPG ': 699, ' JSON ': 292, ' JSONL ': 1, ' KML ': 934, ' KMZ ': 396, ' LAS ': 6, ' LYR ': 5, ' MXD ': 104, ' NetCDF ': 36, ' ODP ': 1, ' ODS ': 19, ' ODT ': 8, ' PDF ': 4490, ' PDF/A-1 ': 22, ' PDF/A-2 ': 2, ' PDF/UA ': 18, ' PNG ': 13, ' PPTX ': 8, ' RDF ': 617, ' RSS ': 861, ' RTF ': 14, ' SAS ': 7, ' SEGY ': 253, ' SHP ': 4335, ' SQL ': 1, ' SQLITE ': 20, ' TAB ': 39, ' TIFF ': 144, ' TXT ': 377, ' VPF ': 1, ' WCS ': 15, ' WFS ': 75, ' WMS ': 1329, ' WMTS ': 10, ' XLS ': 556, ' XLSM ': 1, ' XLSX ': 950, ' XML ': 11303, ' ZIP ': 2763, ' other ': 11099}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"open.canada.ca\", \"country\": \"Canada\", \"num_datasets\": num_datasets}\n",
    "#find all <summary> items with the class \"panel-heading\"\n",
    "for summary in soup.find_all(\"summary\", {\"class\": \"panel-heading\"}):\n",
    "    x = summary.find(\"h5\", {\"class\": \"panel-title\"})\n",
    "    if x.text == \"Formats\":\n",
    "        # get the parent in the soup of this element\n",
    "        panel = summary.parent.find(\"ul\", {\"class\": \"list-group\"})\n",
    "        for row in panel.find_all(\"div\", {\"class\": \"row\"}):\n",
    "            format = row.find(\"label\").text\n",
    "            num = row.find(\"span\", {\"class\": \"badge\"}).text\n",
    "                formats[format] = formats.get(format, 0) + int(num)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/canada_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 9696\n"
     ]
    }
   ],
   "source": [
    "#repeat for mexico\n",
    "query = \"https://datos.gob.mx/busca/dataset?_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "#\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "num_text = soup.find(\"p\", text=lambda text: \"Datos\" in text)\n",
    "num_datasets = int(''.join(filter(str.isdigit, num_text.text)))\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV (4949)\n",
      "4949\n",
      "CSV (delimitado por comas) (9)\n",
      "9\n",
      "{'source': 'datos.gob.mx', 'country': 'Mexico', 'num_datasets': 9696, 'CSV': 4958, 'ZIP': 1580, 'vnd.ms-excel': 979, 'XLS': 877, 'PDF': 820, 'JSON': 816, 'XLSX': 551, 'vnd.google-earth.kml+xml': 159, 'SHP': 140, 'TXT': 109, 'KML': 99, 'vnd.google-earth.kmz': 32, 'XML': 29, 'kmz': 27, 'php': 23, 'DOCX': 22, 'RAR': 20, 'kml': 18, 'KMZ': 16, 'view': 14, 'DOC': 13, 'csv': 12, 'arcGis': 11, 'SQL': 11, 'ArcGIS': 10, 'CSV, EXCEL': 9, 'ODT': 8, 'Word': 8, 'MySQL': 7, 'GeoJSON': 6, 'HTML': 6, 'ODS': 6, 'Oracle': 6, 'ORACLE': 6, 'SHAPE': 6, '.csv': 5, 'oracle': 5, 'SQL Server 2012': 5, 'CSV, XLS, XML': 4, 'Excel': 7, 'Plataforma Joomla, Formato de los archivos .CSV': 4, 'shape': 4, 'ArcGis': 3, 'ARCGIS': 3, 'excel CSV': 3, 'Manejador de Bases de Datos MySQL.': 3, 'shp': 3, 'SQL Server': 3}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"datos.gob.mx\", \"country\": \"Mexico\", \"num_datasets\": num_datasets}\n",
    "# find the <a> tag with the text \"Formatos\"\n",
    "for x in soup.find_all(\"a\"):\n",
    "    if \"Formatos\" in x.text:\n",
    "        if \"Mostrar Solamente Populares\" in x.text:\n",
    "            continue\n",
    "        lst = x.find_next_sibling(\"div\")\n",
    "        for li in lst.find_all(\"li\"):\n",
    "            try:\n",
    "                format_text = li.find(\"a\").text\n",
    "                if \"Mostrar Solamente Populares\" in format_text:\n",
    "                    continue\n",
    "                format = format_text.split(\"(\")[0].strip()\n",
    "                num = format_text.split(\"(\")[1].split(\")\")\n",
    "                #extract the content of the last parenthesis in a string with a regular expression\n",
    "                num = re.findall(r'\\(([^)]+)', format_text)[-1]\n",
    "                formats[format] = formats.get(format, 0) + int(num)\n",
    "            except:\n",
    "                print(\"Error parsing\", format_text)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/mexico_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### South America: Brazil, Argentina, Chile, Colombia, Ecuador\n",
    "\n",
    "- Brazil statistics from https://dados.gov.br\n",
    "- Argentina statistics from https://datos.gob.ar\n",
    "- Chile statistics from https://datos.gob.cl\n",
    "- Colombia statistics from https://datos.gov.co"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 13619\n"
     ]
    }
   ],
   "source": [
    "# repeat for brazil\n",
    "query = \"https://dados.gov.br/dataset?q=&_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "num_text = soup.find(\"h3\", text=lambda text: \"dados encontrado\" in text)\n",
    "num_datasets = int(''.join(filter(str.isdigit, num_text.text)))\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "formats = {\"source\": \"dados.gov.br\", \"country\": \"Brazil\", \"num_datasets\": num_datasets}\n",
    "\n",
    "for section in soup.find_all(\"section\"):\n",
    "    lst = section.find_all(\"h2\")\n",
    "    for h2 in lst:\n",
    "        try:\n",
    "            if \"Formatos\" not in h2.text:\n",
    "                continue\n",
    "            for a in section.find_all(\"a\"):\n",
    "                format_text = a.text\n",
    "                if \"Mostrar somente Formatos popular\" in format_text:\n",
    "                    continue\n",
    "                format = format_text.split(\"(\")[0].strip()\n",
    "                num = format_text.split(\"(\")[1].split(\")\")\n",
    "                #extract the content of the last parenthesis in a string with a regular expression\n",
    "                num = re.findall(r'\\(([^)]+)', format_text)[-1]\n",
    "                formats[format] = formats.get(format, 0) + int(num)\n",
    "        except:\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "with open(\"./raw_data/brazil_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 1129\n"
     ]
    }
   ],
   "source": [
    "# repeat for argentina\n",
    "query = \"http://datos.gob.ar/api/3/action/package_list\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "ar_datasets = r.json()[\"result\"]\n",
    "\n",
    "num_datasets = len(ar_datasets)\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-1a2db76eb67d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjoblib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0mn_jobs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mlst_formats\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjoblib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjoblib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelayed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_format\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mar_datasets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/pollution/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1059\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1060\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1061\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1062\u001B[0m             \u001B[0;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1063\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pollution/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    938\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    939\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'supports_timeout'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 940\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    941\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    942\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pollution/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mwrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    540\u001B[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001B[1;32m    541\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 542\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    543\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mCfTimeoutError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    544\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pollution/lib/python3.8/concurrent/futures/_base.py\u001B[0m in \u001B[0;36mresult\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    432\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__get_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 434\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_condition\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    435\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    436\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_state\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mCANCELLED\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCANCELLED_AND_NOTIFIED\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/pollution/lib/python3.8/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    301\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 302\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    303\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# wrap this function in multiprocessing to speed up the process\n",
    "def get_format(dataset):\n",
    "    formats = {}\n",
    "    query = \"http://datos.gob.ar/api/3/action/package_show?id=\" + dataset\n",
    "    r  =requests.get(query)\n",
    "    assert r.status_code == 200\n",
    "    dataset = r.json()[\"result\"]\n",
    "    if \"resources\" in dataset:\n",
    "        for resource in dataset[\"resources\"]:\n",
    "            if \"format\" in resource:\n",
    "                format = resource[\"format\"]\n",
    "                formats[format] = formats.get(format, 0) + 1\n",
    "    return formats\n",
    "\n",
    "import joblib\n",
    "n_jobs = 10\n",
    "lst_formats = joblib.Parallel(n_jobs=4)(joblib.delayed(get_format)(dataset) for dataset in ar_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "formats = {}\n",
    "for d in lst_formats:\n",
    "    try:\n",
    "        formats = {k: formats.get(k, 0) + d.get(k, 0) for k in set(formats) | set(d)}\n",
    "    except:\n",
    "        print(\"Error parsing\", d)\n",
    "\n",
    "formats[\"source\"] = \"datos.gob.ar\"\n",
    "formats[\"country\"] = \"Argentina\"\n",
    "formats[\"num_datasets\"] = num_datasets\n",
    "\n",
    "with open(\"./raw_data/argentina_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 3591\n"
     ]
    }
   ],
   "source": [
    "#analysis for chile\n",
    "query = \"https://datos.gob.cl/dataset?q=&_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "#\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "num_text = soup.find(\"h2\", text=lambda text: \"datos encontrados\" in text)\n",
    "num_datasets = int(''.join(filter(str.isdigit, num_text.text)))\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "formats = {\"source\": \"datos.gob.cl\", \"country\": \"Chile\", \"num_datasets\": num_datasets}\n",
    "# find the <a> tag with the text \"Formatos\"\n",
    "for x in soup.find_all(\"h3\"):\n",
    "    if \"Formatos\" not in x.text:\n",
    "        continue\n",
    "    nav = x.find_next_sibling(\"nav\")\n",
    "    for li in nav.find_all(\"li\"):\n",
    "        try:\n",
    "            if \"populares\" in format_text:\n",
    "                continue\n",
    "            format_text = li.find(\"a\").text\n",
    "            format = \" \".join(format_text.strip().split()[:-1])\n",
    "            num = format_text.strip().split()[-1]\n",
    "            formats[format] = formats.get(format, 0) + int(num)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "with open(\"./raw_data/chile_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Europe: UK, Germany, Spain\n",
    "\n",
    "- UK statistics from https://data.gov.uk\n",
    "- Germany statistics from https://www.govdata.de\n",
    "- Spain statistics from https://datos.gob.es"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 52927\n"
     ]
    }
   ],
   "source": [
    "# analysis with uk website\n",
    "query = \"https://data.gov.uk/api/3/action/package_list\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "uk_datasets = r.json()[\"result\"]\n",
    "num_datasets = len(uk_datasets)\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "QUEUEING TASKS | :   0%|          | 0/52927 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0200635e6f1549498262e0d31ef7f861"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "PROCESSING TASKS | :   0%|          | 0/52927 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76d66aad804643e68de6a14388dec1f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-24:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "def get_format(dataset):\n",
    "    formats = {}\n",
    "    query = \"https://data.gov.uk/api/3/action/package_show?id=\" + dataset\n",
    "    r  =requests.get(query)\n",
    "    assert r.status_code == 200\n",
    "    dataset = r.json()[\"result\"]\n",
    "    if \"resources\" in dataset:\n",
    "        for resource in dataset[\"resources\"]:\n",
    "            if \"format\" in resource:\n",
    "                format = resource[\"format\"]\n",
    "                formats[format] = formats.get(format, 0) + 1\n",
    "    return formats\n",
    "\n",
    "import joblib\n",
    "n_jobs = 10\n",
    "# lst_formats = joblib.Parallel(n_jobs=12)(joblib.delayed(get_format)(dataset) for dataset in uk_datasets)\n",
    "from pqdm.processes import pqdm\n",
    "result = pqdm([dataset for dataset in uk_datasets], get_format, n_jobs=n_jobs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "lst_formats = result\n",
    "formats = {}\n",
    "for d in lst_formats:\n",
    "    try:\n",
    "        formats = {k: formats.get(k, 0) + d.get(k, 0) for k in set(formats) | set(d)}\n",
    "    except:\n",
    "        print(\"Error parsing\", d)\n",
    "\n",
    "formats[\"source\"] = \"data.gov.uk\"\n",
    "formats[\"country\"] = \"United Kingdom\"\n",
    "formats[\"num_datasets\"] = num_datasets\n",
    "\n",
    "with open(\"./raw_data/uk_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 63470\n"
     ]
    }
   ],
   "source": [
    "# repeat for Germany\n",
    "query = \"https://www.govdata.de/web/guest/suchen/-/searchresult/s/relevance_desc\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "num_text = soup.find(\"h2\", {\"class\": \"hitscount\"})\n",
    "num_datasets = int(''.join(filter(str.isdigit, num_text.text)))\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23285\n",
      "23285\n",
      "{'source': 'www.govdata.de', 'country': 'Germany', 'num_datasets': 63470, 'csv': 23285, 'pdf': 12461, '': 10686, 'html': 9437, 'wms': 6517, 'karte': 5868, 'webanwendung': 5867, 'zip': 5196, 'xlsx': 5168, 'json': 3145}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"www.govdata.de\", \"country\": \"Germany\", \"num_datasets\": num_datasets}\n",
    "# find the <dl> tag that contains the <dt> tag with the class \"filtergroup-title\" and the text \"Dateiformat\"\n",
    "for x in soup.find_all(\"dl\"):\n",
    "    filtergroups = x.find(\"dt\", {\"class\": \"filtergroup-title\"})\n",
    "    if not filtergroups or \"Dateiformat\" not in filtergroups.text:\n",
    "        continue\n",
    "    for dd in x.find_all(\"dd\"):\n",
    "        try:\n",
    "            format_link = dd.find(\"a\")\n",
    "            # format is the <div> inside the <a> tag with the class \"filtername\"\n",
    "            if not \"weitere anzeigen\" not in format_link.text:\n",
    "                continue\n",
    "            format = format_link.find(\"div\", {\"class\": \"filtername\"}).text\n",
    "            num = int(format_link.find(\"div\", {\"class\": \"filterresultcount\"}).text)\n",
    "            formats[format] = formats.get(format, 0) + int(num)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # print(\"Error parsing\", format_text)\n",
    "\n",
    "with open(\"./raw_data/germany_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 61761\n"
     ]
    }
   ],
   "source": [
    "# repeat for spain\n",
    "query = \"https://datos.gob.es/es/catalogo?_res_format_label_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    try:\n",
    "        if \"datos encontrados\" in h2.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, h2.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "formats = {\"source\": \"datos.gob.es\", \"country\": \"Spain\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Formato\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Formato\" not in x.text:\n",
    "        continue\n",
    "    nav = x.parent.find_next_sibling(\"nav\")\n",
    "    for li in nav.find_all(\"li\"):\n",
    "        try:\n",
    "            format_text = li.find(\"a\").text\n",
    "            if \"populares\" in format_text:\n",
    "                continue\n",
    "            format = \" \".join(format_text.strip().split()[:-1])\n",
    "            # get the number that is inbetween brackets in a string\n",
    "            num = int(re.search(r'\\((\\d+)\\)', format_text).group(1))\n",
    "            formats[format] = formats.get(format, 0) + int(num)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "with open(\"./raw_data/spain_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Asia: Pakistan, Japan\n",
    "\n",
    "- Pakistan statistics from https://opendata.com.pk\n",
    "- Japan statistics from https://www.data.go.jp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# analysis for pakistan\n",
    "query = \"https://opendata.com.pk/dataset?_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    try:\n",
    "        if \"datasets found\" in h2.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, h2.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%#repeat for asia\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "formats = {\"source\": \"opendata.com.pk\", \"country\": \"Pakistan\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Format\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Formats\" not in x.text:\n",
    "        continue\n",
    "    nav = x.parent.find(\"nav\")\n",
    "    for li in nav.find_all(\"li\"):\n",
    "        try:\n",
    "            format = li.find(\"span\", {\"class\":\"item-label\"}).text\n",
    "            if \"Popular\" in format_text:\n",
    "                continue\n",
    "            num_text = li.find(\"span\", {\"class\":\"item-count\"}).text\n",
    "            num = int(re.search(r'\\((\\d+)\\)', num_text).group(1))\n",
    "            formats[format] = formats.get(format, 0) + int(num)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "with open(\"./raw_data/pakistan_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 22944\n"
     ]
    }
   ],
   "source": [
    "# analysis for japan\n",
    "query = \"https://www.data.go.jp/data/en/dataset?q=&_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    try:\n",
    "        if \"datasets found\" in h2.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, h2.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data.go.jp', 'country': 'Japan', 'num_datasets': 22944, 'PDF': 10566, 'HTML': 6678, 'XLS': 5442, 'XLSX': 1802, 'ZIP': 801, 'CSV': 657, 'html': 523, 'pdf': 507, 'JPEG': 414, 'csv': 365, 'XML': 160, 'xls': 147, 'GIF': 90, 'KMZ': 73, 'PNG': 47, 'EXE': 36, 'TXT': 35, 'DOC': 30, 'DOCX': 24, 'xlsx': 17, 'PPTX': 12, 'lzh': 11, 'epub': 10, 'KML': 10, 'PPT': 7, 'asx': 6, 'mp3': 6, 'jtd': 5, 'php': 5, 'jsp': 1, 'ODT': 1, 'SHP': 1, 'zip': 1}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"data.go.jp\", \"country\": \"Japan\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Format\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Formats\" not in x.text:\n",
    "        continue\n",
    "    nav = x.parent.find(\"nav\")\n",
    "    for li in nav.find_all(\"li\"):\n",
    "        try:\n",
    "            format = li.find(\"span\", {\"class\":\"item-label\"}).text\n",
    "            if \"Popular\" in format_text:\n",
    "                continue\n",
    "            num_text = int(li.find(\"span\", {\"class\":\"item-count\"}).text)\n",
    "            formats[format] = formats.get(format, 0) + int(num_text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "with open(\"./raw_data/japan_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Oceania: Australia, New Zealand\n",
    "\n",
    "- Australia statistics from https://data.gov.au\n",
    "- New Zealand statistics from https://catalogue.data.govt.nz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# analysis for australia\n",
    "query = \"https://data.gov.au/api/v0/search/datasets?start=0&limit=11&publishingState=published\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "response = r.json()\n",
    "num_datasets = response[\"hitCount\"]\n",
    "print(\"Total number of datasets\", num_datasets)\n",
    "\n",
    "formats = {\"source\": \"https://data.gov.au\", \"country\": \"Australia\", \"num_datasets\": num_datasets}\n",
    "for x in response[\"facets\"]:\n",
    "    if x[\"id\"] == \"Format\":\n",
    "        for f in x[\"options\"]:\n",
    "            format_name = f[\"value\"]\n",
    "            format_count = f[\"hitCount\"]\n",
    "            formats[format_name] = int(format_count)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/australia_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 31881\n"
     ]
    }
   ],
   "source": [
    "# analysis for new zealand\n",
    "query = \"https://catalogue.data.govt.nz/dataset/?q=&_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    try:\n",
    "        if \"datasets found\" in h2.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, h2.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data.govt.nz', 'country': 'New Zealand', 'num_datasets': 31881, 'KML': 18272, 'CSV': 16126, 'PDF': 14192, 'HTML': 13469, 'DWG': 12737, 'SHP': 10578, 'FileGDB': 10304, 'GPKG': 10304, 'MapInfo File': 10304, 'TIFF': 6975, 'GeoJSON': 5586, 'ZIP': 5553, 'GTiff': 4245, 'KEA': 4245, 'ArcGIS GeoServices ...': 4229, 'HFA': 3213, 'JPEG': 3211, 'JP2KAK': 3210, 'JP2KAK_LOSSLESS': 3210, 'MapInfo MIF': 2835, 'Esri REST': 2025, 'AAIGrid': 1032, 'XLSX': 564, 'XLS': 455, 'OGC WMS': 388, 'OGC WFS': 268, 'File Geodatabase Fe...': 184, '.xlsx': 118, 'API': 67, 'GIS': 59, 'Mixed': 54, 'Mesh Dataset': 52, 'RasterDataset': 49, 'Word': 39, 'File Geodatabase Ra...': 35, '.pdf': 31, 'excel workbook (*.x...': 30, 'GeoService API, Geo...': 29, 'XML': 27, 'xlsm': 23, 'Raster Dataset': 22, 'Zip/CSV': 16, 'geoservice api, geo...': 15, 'TXT': 13, 'XLSM': 13, '.csv': 12, 'SDE Feature Class': 12, '.xls': 10, 'Zip/Spreadsheet': 10, '.xlxs': 9}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"data.govt.nz\", \"country\": \"New Zealand\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Format\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Formats\" not in x.text:\n",
    "        continue\n",
    "    nav = x.parent.find(\"nav\")\n",
    "    for li in nav.find_all(\"li\"):\n",
    "        try:\n",
    "            format = li.find(\"span\", {\"class\":\"item-label\"}).text\n",
    "            if \"Popular\" in format_text:\n",
    "                continue\n",
    "            num_text = int(li.find(\"span\", {\"class\":\"item-count\"}).text)\n",
    "            formats[format] = formats.get(format, 0) + int(num_text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/new_zealand_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Africa: Morocco, Ghana, Somalia\n",
    "\n",
    "- Morocco statistics from https://data.gov.ma\n",
    "- Ghana statistics from https://data.gov.gh\n",
    "- Somalia statistics from"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# repeat for morocco\n",
    "query = \"https://data.gov.ma/data/fr/dataset?q=&_res_format_limit=0\"\n",
    "r  =requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    try:\n",
    "        if \"données trouvés\" in h2.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, h2.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data.gov.ma', 'country': 'Morocco', 'num_datasets': 31881, 'XLSX': 212, 'XLS': 100, 'DOCX': 54, 'DOC': 16, 'CSV': 10, 'PDF': 6, 'PPTX': 6, '.docx': 1}\n"
     ]
    }
   ],
   "source": [
    "formats = {\"source\": \"data.gov.ma\", \"country\": \"Morocco\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Format\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Formats\" not in x.text:\n",
    "        continue\n",
    "    nav = x.parent.find(\"nav\")\n",
    "    for li in nav.find_all(\"li\"):\n",
    "        try:\n",
    "            format = li.find(\"span\", {\"class\":\"item-label\"}).text\n",
    "            num_text = int(li.find(\"span\", {\"class\":\"item-count\"}).text)\n",
    "            formats[format] = formats.get(format, 0) + int(num_text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error parsing\", format_text)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/morocco_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%formats = {\"source\": \"data.gov.ma\", \"country\": \"Morocco\", \"num_datasets\": num_datasets}\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 1308\n",
      "{'source': 'data.gov.gh', 'country': 'Ghana', 'num_datasets': 1308, 'csv': 263, 'excel': 19, 'arcgis': 10, 'pdf': 7, 'xlsx': 4, 'argis': 1, 'xlb': 1, 'zip': 1}\n"
     ]
    }
   ],
   "source": [
    "# analysis for ghana\n",
    "query = \"https://data.gov.gh/search?sort_by=changed\"\n",
    "r = requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for div in soup.find_all(\"div\"):\n",
    "    try:\n",
    "        if \"results\" in div.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, div.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)\n",
    "formats = {\"source\": \"data.gov.gh\", \"country\": \"Ghana\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Format\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Format\" not in x.text:\n",
    "        continue\n",
    "    div = x.find_next_sibling(\"div\")\n",
    "    for li in div.find_all(\"a\"):\n",
    "        format_text =li.text\n",
    "        format = format_text.split(\" (\")[0]\n",
    "        num = int(re.search(r'\\((\\d+)\\)', format_text).group(1))\n",
    "        formats[format] = formats.get(format, 0) + int(num)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/ghana_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets 8\n",
      "{'source': 'www.data.gov.so', 'country': 'Somalia', 'num_datasets': 8, 'PDF': 8, 'CSV': 2, 'XLS': 1, 'XLSX': 1}\n"
     ]
    }
   ],
   "source": [
    "# analysis for ghana\n",
    "query = \"https://www.data.gov.so/dataset\"\n",
    "r = requests.get(query)\n",
    "assert r.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "for h in soup.find_all(\"h2\"):\n",
    "    try:\n",
    "        if \"datasets found\" in h.text.strip():\n",
    "            num_datasets = int(''.join(filter(str.isdigit, h.text)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "print(\"Total number of datasets\", num_datasets)\n",
    "formats = {\"source\": \"www.data.gov.so\", \"country\": \"Somalia\", \"num_datasets\": num_datasets}\n",
    "# find the <h2> tag that contains \"Format\" and get the next \"nav\" element\n",
    "for x in soup.find_all(\"h2\"):\n",
    "    if \"Format\" not in x.text:\n",
    "        continue\n",
    "    nav = x.find_next_sibling(\"nav\")\n",
    "    for li in nav.find_all(\"a\"):\n",
    "        format_text =li.text\n",
    "        format = format_text.strip().split(\" (\")[0]\n",
    "        num = int(re.search(r'\\((\\d+)\\)', format_text).group(1))\n",
    "        formats[format] = formats.get(format, 0) + int(num)\n",
    "\n",
    "print(formats)\n",
    "with open(\"./raw_data/somalia_formats.json\", \"w\") as f:\n",
    "    json.dump(formats, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
